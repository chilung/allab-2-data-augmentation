{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_image(src_root, dest_root):\n",
    "    for root, dirs, files in os.walk(src_root, topdown=False):\n",
    "        for name in files:\n",
    "            src_fullpath = os.path.join(src_root, name)\n",
    "            #print(src_fullpath)\n",
    "            \n",
    "            img = Image.open(src_fullpath)\n",
    "            img = transforms.functional.resize(img, (224, 224))\n",
    "            #img = transforms.functional.to_grayscale(img, 3)\n",
    "            \n",
    "            dest_fullpath = os.path.join(dest_root, name)\n",
    "            #print(dest_fullpath)\n",
    "                \n",
    "            img.save(dest_fullpath)\n",
    "            \n",
    "            #for angle in range(-60, 90, 30):\n",
    "            #    imgr = torchvision.transforms.functional.rotate(img, angle, expand=True)\n",
    "                \n",
    "            #    filename, file_extension = os.path.splitext(name)\n",
    "            #    dest_fullpath = os.path.join(dest_root, filename+'_'+str(angle)+file_extension)\n",
    "            #    print(dest_fullpath)\n",
    "                \n",
    "            #    imgr.save(dest_fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_training_data_path = \"../data/src_food/training\"\n",
    "src_evaluation_data_path = \"../data/src_food/evaluation\"\n",
    "src_validation_data_path = \"../data/src_food/validation\"\n",
    "\n",
    "dest_training_data_path = \"../data/temp/training\"\n",
    "dest_evaluation_data_path = \"../data/temp/evaluation\"\n",
    "dest_validation_data_path = \"../data/temp/validation\"\n",
    "\n",
    "#transform_image(src_training_data_path, dest_training_data_path)\n",
    "#transform_image(src_evaluation_data_path, dest_evaluation_data_path)\n",
    "#transform_image(src_validation_data_path, dest_validation_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:, i, :, :].mean()\n",
    "            std[i] += inputs[:, i, :, :].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Check devices..\n",
      "Current device:  cuda\n",
      "Our selected device:  0\n",
      "1  GPUs is available\n"
     ]
    }
   ],
   "source": [
    "#To determine if your system supports CUDA\n",
    "print(\"==> Check devices..\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Current device: \",device)\n",
    "\n",
    "#Also can print your current GPU id, and the number of GPUs you can use.\n",
    "print(\"Our selected device: \", torch.cuda.current_device())\n",
    "print(torch.cuda.device_count(), \" GPUs is available\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing dataset..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing mean and std..\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/datasets/folder.py\", line 103, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/functional.py\", line 204, in resize\n    return img.resize((ow, oh), interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/PIL/Image.py\", line 1695, in resize\n    raise ValueError(\"unknown resampling filter\")\nValueError: unknown resampling filter\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-7221ef279604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../data/food/evaluation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrain_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_and_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mval_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_and_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-fc8187d4f1da>\u001b[0m in \u001b[0;36mget_mean_and_std\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==> Computing mean and std..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/datasets/folder.py\", line 103, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/functional.py\", line 204, in resize\n    return img.resize((ow, oh), interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/PIL/Image.py\", line 1695, in resize\n    raise ValueError(\"unknown resampling filter\")\nValueError: unknown resampling filter\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1.1\"\"\"\n",
    "#img_size, linear_size, fc1_out, fc2_out = 64, 13, 128, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 64, 13, 128, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 128, 29, 256, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 256, 61, 256, 84\n",
    "img_size, linear_size, fc1_out, fc2_out = 224, 53, 120, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 512, 125, 256, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 512, 125, 512, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 1024, 253, 256, 84\n",
    "\n",
    "calculate_mean_std = True\n",
    "\n",
    "if calculate_mean_std == True:\n",
    "    #The transform function for train data\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(img_size, img_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize(img_size, img_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    #The transform function for test data\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(img_size, img_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    #we will calculate mean and std\n",
    "    \n",
    "    trainset = torchvision.datasets.ImageFolder(root='../data/food/training', transform=transform_train)\n",
    "    valset = torchvision.datasets.ImageFolder(root='../data/food/validation', transform=transform_val)\n",
    "    testset = torchvision.datasets.ImageFolder(root='../data/food/evaluation', transform=transform_test)\n",
    "    \n",
    "    train_mean, train_std = get_mean_and_std(trainset)\n",
    "    print(train_mean, train_std)\n",
    "    val_mean, val_std = get_mean_and_std(valset)\n",
    "    print(val_mean, val_std)\n",
    "    test_mean, test_std = get_mean_and_std(testset)\n",
    "    print(test_mean, test_std)\n",
    "else:\n",
    "    train_mean, train_std = ([0.4677, 0.4677, 0.4677]), ([0.2274, 0.2274, 0.2274])\n",
    "    val_mean, val_std = ([0.4718, 0.4718, 0.4718]), ([0.2260, 0.2260, 0.2260])\n",
    "    test_mean, test_std = ([0.4735, 0.4735, 0.4735]), ([0.2275, 0.2275, 0.2275])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"1.1+\"\"\"\n",
    "#The transform function for train data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(img_size, img_size),\n",
    "    #transforms.RandomCrop(img_size, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "])\n",
    "\n",
    "#The transform function for validation data\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(img_size, img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(val_mean, val_std)\n",
    "])\n",
    "\n",
    "#The transform function for test data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(img_size, img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean, test_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"1.2+\"\"\"\n",
    "trainset = torchvision.datasets.ImageFolder(root='../data/food/training', transform=transform_train)\n",
    "valset = torchvision.datasets.ImageFolder(root='../data/food/validation', transform=transform_val)\n",
    "testset = torchvision.datasets.ImageFolder(root='../data/food/evaluation', transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"1.3\"\"\"\n",
    "\n",
    "#Create DataLoader to draw samples from the dataset\n",
    "#In this case, we define a DataLoader to random sample our dataset. \n",
    "#For single sampling, we take one batch of data. Each batch consists 4 images\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "shuffle=True, num_workers=2)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32,\n",
    "shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('Bread', 'DairyProduct', 'Dessert', 'Egg', 'Friedfood',\n",
    "           'Meat', 'Noodles-Pasta', 'Rice', 'Seafood', 'Soup', 'Vegetable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Building model..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define your own model\n",
    "class Net(nn.Module):\n",
    "\n",
    "    #define the layers\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * linear_size * linear_size, fc1_out)\n",
    "        self.fc2 = nn.Linear(fc1_out, fc2_out)\n",
    "        self.fc3 = nn.Linear(fc2_out, 11)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    #concatenate these layers\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        #x = self.dropout(x)\n",
    "        x = x.view(-1, 16 * linear_size * linear_size)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#declare a new model\n",
    "#net = Net()\n",
    "import torchvision.models as models\n",
    "net = models.resnet18(pretrained=False)\n",
    "\n",
    "# change all model tensor into cuda type\n",
    "# something like weight & bias are the tensor \n",
    "#net = net.to(device)\n",
    "print(device)\n",
    "if device == 'cuda':\n",
    "    net = net.cuda(0)\n",
    "else:\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "\n",
    "# 3. Define a Loss function and optimize\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Defining loss function and optimize..\n"
     ]
    }
   ],
   "source": [
    "print('==> Defining loss function and optimize..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimization algorithm\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "\n",
    "# 4. Train the network\n",
    "\n",
    "Before training the model, we need to analysis the tensor variable.\n",
    "\n",
    "\n",
    "Each variable have many attibute, like: .grad_fn, .require_grad, .data, .grad...etc. The \".grad_fn\" attribute of \"torch.Tensor\" is an entry point into the function that has create this \"torch.Tensor\" variables. Because of \".grad_fn\" flag, we can easily create a computing graph in the form of DAG(directed acyclic graph).\n",
    "\n",
    "And then, the \".require_grad\" attribute allows us to determine whether the backward propagation function is going to calculate the gradient of this \"torch.Tensor\" variable. If one variable has a false value of require_grad, it represent that you don't want to calculate this variable's gradient, and also its gradient will not be updated.\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Training model..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "for phase in ['train', 'val']:\n",
    "    if phase == 'train':\n",
    "        print(phase)    \n",
    "    else:\n",
    "        print(phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/datasets/folder.py\", line 103, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/functional.py\", line 204, in resize\n    return img.resize((ow, oh), interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/PIL/Image.py\", line 1695, in resize\n    raise ValueError(\"unknown resampling filter\")\nValueError: unknown resampling filter\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-ce5e905d7887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdset_loaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/datasets/folder.py\", line 103, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/functional.py\", line 204, in resize\n    return img.resize((ow, oh), interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/PIL/Image.py\", line 1695, in resize\n    raise ValueError(\"unknown resampling filter\")\nValueError: unknown resampling filter\n"
     ]
    }
   ],
   "source": [
    "#Set the model in training mode\n",
    "#because some function like: dropout, batchnorm...etc, will have \n",
    "#different behaviors in training/evaluation mode\n",
    "#[document]: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train\n",
    "import copy\n",
    "\n",
    "best_model = net\n",
    "best_acc = 0.0\n",
    "\n",
    "net.train()\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma=0.5)\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    #scheduler.step()\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            net.train(True)  # Set model to training mode\n",
    "            dset_loaders = trainloader\n",
    "        else:\n",
    "            net.train(False)  # Set model to evaluate mode\n",
    "            dset_loaders = valloader\n",
    "            \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        \n",
    "        for data in dset_loaders:\n",
    "        \n",
    "            (inputs, labels) = data\n",
    "            \n",
    "            #change the type into cuda tensor \n",
    "            if device == 'cuda':\n",
    "                inputs = inputs.cuda(0)\n",
    "                labels = labels.cuda(0)\n",
    "            else:\n",
    "                inputs = inputs.cpu()\n",
    "                labels = labels.cpu()\n",
    "\n",
    "            #print(labels)\n",
    "            #print(inputs)\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            # select the class with highest probability\n",
    "            _, pred = outputs.max(1)\n",
    "            # if the model predicts the same results as the true\n",
    "            # label, then the correct counter will plus 1\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if phase == 'train':\n",
    "            epoch_loss = running_loss / len(trainset)\n",
    "            epoch_acc = correct / len(trainset)\n",
    "        else:\n",
    "            epoch_loss = running_loss / len(valset)\n",
    "            epoch_acc = correct / len(valset)\n",
    "\n",
    "        print('{:d} Epoch {} Loss: {:.4f} Acc: {:.4f}'.format(epoch, phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(net)\n",
    "print('Finished Training')\n",
    "\n",
    "net = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "def show_weight_distribution(epoch):\n",
    "    n_bins = 50\n",
    "    font_size = 8\n",
    "    plt.rcParams.update({'axes.titlesize': font_size})\n",
    "    plt.rcParams.update({'axes.labelsize': font_size})\n",
    "    plt.rcParams.update({'xtick.labelsize': font_size})\n",
    "    plt.rcParams.update({'ytick.labelsize': font_size})\n",
    "    fig, axs = plt.subplots(1, 5, sharey=True)\n",
    "    \n",
    "    x = net.conv1.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('conv1')\n",
    "    plt.title('Weight Distribution, epoch: %d' %(epoch))\n",
    "\n",
    "    x = net.conv2.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('conv2')\n",
    "\n",
    "    x = net.fc1.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc1')\n",
    "\n",
    "    x = net.fc2.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc2')\n",
    "\n",
    "    x = net.fc3.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc3')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "def show_bias_distribution(epoch):\n",
    "    n_bins = 50\n",
    "    font_size = 8\n",
    "    plt.rcParams.update({'axes.titlesize': font_size})\n",
    "    plt.rcParams.update({'axes.labelsize': font_size})\n",
    "    plt.rcParams.update({'xtick.labelsize': font_size})\n",
    "    plt.rcParams.update({'ytick.labelsize': font_size})\n",
    "    fig, axs = plt.subplots(1, 5, sharey=True)\n",
    "    \n",
    "    x = net.conv1.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('conv1')\n",
    "    plt.title('Bias Distribution, epoch: %d' %(epoch))\n",
    "\n",
    "    x = net.conv2.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('conv2')\n",
    "\n",
    "    x = net.fc1.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc1')\n",
    "\n",
    "    x = net.fc2.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc2')\n",
    "\n",
    "    x = net.fc3.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc3')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'conv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f17b1820c3fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_weight_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_bias_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-83fc42c55b8e>\u001b[0m in \u001b[0;36mshow_weight_distribution\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Weight Distribution, epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'conv2'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAERCAYAAACdPxtnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDTMEJKlhhoAkNcwQkKSGGQKS1LD/B5rO/2GV690BAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_weight_distribution(epoch+1)\n",
    "show_bias_distribution(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving model..\n",
      "Finished Saving\n"
     ]
    }
   ],
   "source": [
    "#After training , save the model first\n",
    "#You can saves only the model parameters or entire model\n",
    "#Some difference between the two is that entire model \n",
    "#not only include parameters but also record hwo each \n",
    "#layer is connected(forward method).\n",
    "#[document]: https://pytorch.org/docs/master/notes/serialization.html\n",
    "\n",
    "print('==> Saving model..')\n",
    "\n",
    "#only save model parameters\n",
    "torch.save(net.state_dict(), './checkpoint.t7')\n",
    "#you also can store some log information\n",
    "state = {\n",
    "    'net': net.state_dict(),\n",
    "    'acc': 100.*correct/len(trainset),\n",
    "    'epoch': 75\n",
    "}\n",
    "torch.save(state, './checkpoint.t7')\n",
    "\n",
    "#save entire model\n",
    "torch.save(net, './model.pt')\n",
    "\n",
    "print('Finished Saving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "\n",
    "# 5. Test the network on the test data\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading model..\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"conv1.bias\", \"conv2.bias\", \"conv2.weight\", \"fc1.bias\", \"fc1.weight\", \"fc2.bias\", \"fc2.weight\", \"fc3.bias\", \"fc3.weight\". \n\tUnexpected key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.downsample.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.downsample.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"fc.weight\", \"fc.bias\". \n\tsize mismatch for conv1.weight: copying a param of torch.Size([6, 3, 5, 5]) from checkpoint, where the shape is torch.Size([64, 3, 7, 7]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-3995245619f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoint.t7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#If you save the entire model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"conv1.bias\", \"conv2.bias\", \"conv2.weight\", \"fc1.bias\", \"fc1.weight\", \"fc2.bias\", \"fc2.weight\", \"fc3.bias\", \"fc3.weight\". \n\tUnexpected key(s) in state_dict: \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"bn1.num_batches_tracked\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.bn1.num_batches_tracked\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.bn2.num_batches_tracked\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.bn1.num_batches_tracked\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.bn2.num_batches_tracked\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.bn1.num_batches_tracked\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.bn2.num_batches_tracked\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.downsample.1.num_batches_tracked\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.bn1.num_batches_tracked\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.bn2.num_batches_tracked\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.bn1.num_batches_tracked\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.bn2.num_batches_tracked\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.downsample.1.num_batches_tracked\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.bn1.num_batches_tracked\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.bn2.num_batches_tracked\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.bn1.num_batches_tracked\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.bn2.num_batches_tracked\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.downsample.1.num_batches_tracked\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.bn1.num_batches_tracked\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.bn2.num_batches_tracked\", \"fc.weight\", \"fc.bias\". \n\tsize mismatch for conv1.weight: copying a param of torch.Size([6, 3, 5, 5]) from checkpoint, where the shape is torch.Size([64, 3, 7, 7]) in current model."
     ]
    }
   ],
   "source": [
    "#Before testing, we can load the saved model\n",
    "#Depend on how you save your model, need \n",
    "#different way to use it\n",
    "\n",
    "print('==> Loading model..')\n",
    "\n",
    "#If you just save the model parameters, you\n",
    "#need to redefine the model architecture, and\n",
    "#load the parameters into your model\n",
    "net = Net()\n",
    "checkpoint = torch.load('./checkpoint.t7')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#If you save the entire model\n",
    "net = torch.load('./model.pt')\n",
    "\n",
    "print('Finished Loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Testing model..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=44944, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=11, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('==> Testing model..')\n",
    "\n",
    "#Set the model in evaluation mode\n",
    "#[document]: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval \n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/datasets/folder.py\", line 103, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/functional.py\", line 204, in resize\n    return img.resize((ow, oh), interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/PIL/Image.py\", line 1695, in resize\n    raise ValueError(\"unknown resampling filter\")\nValueError: unknown resampling filter\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-73666b7ddbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/datasets/folder.py\", line 103, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/transforms/functional.py\", line 204, in resize\n    return img.resize((ow, oh), interpolation)\n  File \"/home/ccma/anaconda3/lib/python3.5/site-packages/PIL/Image.py\", line 1695, in resize\n    raise ValueError(\"unknown resampling filter\")\nValueError: unknown resampling filter\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#######LAB 1-1########\n",
    "\"\"\"\n",
    "To Do Here\n",
    "\n",
    "        You should complete the testing step in LAB 1-1\n",
    "You should show the total accuracy & loss [10000 cifar10 test cases]\n",
    "  You also need to tell us, how about the accuracy for each class\n",
    " \n",
    "    For example: Total accuracy is: 60.0% and loss is: 0.02  \n",
    "                 For each class in cifar 10:\n",
    "                 Accuracy of plane : 58.0%\n",
    "                 Accuracy of   car : 22.4%\n",
    "                            .\n",
    "                            .\n",
    "                            .\n",
    "\"\"\"\n",
    "######################\n",
    "correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "class_correct = list(0. for i in range(11))\n",
    "class_total = list(0. for i in range(11))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "\n",
    "        if device == 'cuda':\n",
    "            images = images.cuda(0)\n",
    "            labels = labels.cuda(0)\n",
    "        else:\n",
    "            images = images.cpu()\n",
    "            labels = labels.cpu()\n",
    "        \n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        c = (predicted == labels).squeeze()\n",
    "        #print(predicted)\n",
    "        #print(labels.size(0))\n",
    "        #print(c)\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print('Accuracy of the network on the %d test images: %.2f%%, and loss is: %.3f'\n",
    "      % (total, 100 * correct / total, running_loss / total))\n",
    "\n",
    "for i in range(11):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
