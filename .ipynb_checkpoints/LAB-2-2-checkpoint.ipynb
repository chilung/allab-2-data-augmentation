{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_image(src_root, dest_root):\n",
    "    for root, dirs, files in os.walk(src_root, topdown=False):\n",
    "        for name in files:\n",
    "            src_fullpath = os.path.join(src_root, name)\n",
    "            #print(src_fullpath)\n",
    "            \n",
    "            img = Image.open(src_fullpath)\n",
    "            img = transforms.functional.resize(img, (224, 224))\n",
    "            #img = transforms.functional.to_grayscale(img, 3)\n",
    "            \n",
    "            dest_fullpath = os.path.join(dest_root, name)\n",
    "            #print(dest_fullpath)\n",
    "                \n",
    "            img.save(dest_fullpath)\n",
    "            \n",
    "            #for angle in range(-60, 90, 30):\n",
    "            #    imgr = torchvision.transforms.functional.rotate(img, angle, expand=True)\n",
    "                \n",
    "            #    filename, file_extension = os.path.splitext(name)\n",
    "            #    dest_fullpath = os.path.join(dest_root, filename+'_'+str(angle)+file_extension)\n",
    "            #    print(dest_fullpath)\n",
    "                \n",
    "            #    imgr.save(dest_fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_training_data_path = \"../data/src_food/training\"\n",
    "src_evaluation_data_path = \"../data/src_food/evaluation\"\n",
    "src_validation_data_path = \"../data/src_food/validation\"\n",
    "\n",
    "dest_training_data_path = \"../data/temp/training\"\n",
    "dest_evaluation_data_path = \"../data/temp/evaluation\"\n",
    "dest_validation_data_path = \"../data/temp/validation\"\n",
    "\n",
    "#transform_image(src_training_data_path, dest_training_data_path)\n",
    "#transform_image(src_evaluation_data_path, dest_evaluation_data_path)\n",
    "#transform_image(src_validation_data_path, dest_validation_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:, i, :, :].mean()\n",
    "            std[i] += inputs[:, i, :, :].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Check devices..\n",
      "Current device:  cuda\n",
      "Our selected device:  0\n",
      "1  GPUs is available\n"
     ]
    }
   ],
   "source": [
    "#To determine if your system supports CUDA\n",
    "print(\"==> Check devices..\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Current device: \",device)\n",
    "\n",
    "#Also can print your current GPU id, and the number of GPUs you can use.\n",
    "print(\"Our selected device: \", torch.cuda.current_device())\n",
    "print(torch.cuda.device_count(), \" GPUs is available\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset..\n"
     ]
    }
   ],
   "source": [
    "print('==> Preparing dataset..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Computing mean and std..\n",
      "tensor([0.5548, 0.4508, 0.3435]) tensor([0.2281, 0.2384, 0.2376])\n",
      "==> Computing mean and std..\n",
      "tensor([0.5604, 0.4540, 0.3481]) tensor([0.2260, 0.2367, 0.2352])\n",
      "==> Computing mean and std..\n",
      "tensor([0.5616, 0.4558, 0.3502]) tensor([0.2274, 0.2386, 0.2380])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1.1\"\"\"\n",
    "#img_size, linear_size, fc1_out, fc2_out = 64, 13, 128, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 64, 13, 128, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 128, 29, 256, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 256, 61, 256, 84\n",
    "img_size, linear_size, fc1_out, fc2_out = 224, 53, 120, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 512, 125, 256, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 512, 125, 512, 84\n",
    "#img_size, linear_size, fc1_out, fc2_out = 1024, 253, 256, 84\n",
    "\n",
    "calculate_mean_std = True\n",
    "\n",
    "if calculate_mean_std == True:\n",
    "    #The transform function for train data\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    #The transform function for test data\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    #we will calculate mean and std\n",
    "    \n",
    "    trainset = torchvision.datasets.ImageFolder(root='../data/food/training', transform=transform_train)\n",
    "    valset = torchvision.datasets.ImageFolder(root='../data/food/validation', transform=transform_val)\n",
    "    testset = torchvision.datasets.ImageFolder(root='../data/food/evaluation', transform=transform_test)\n",
    "    \n",
    "    train_mean, train_std = get_mean_and_std(trainset)\n",
    "    print(train_mean, train_std)\n",
    "    val_mean, val_std = get_mean_and_std(valset)\n",
    "    print(val_mean, val_std)\n",
    "    test_mean, test_std = get_mean_and_std(testset)\n",
    "    print(test_mean, test_std)\n",
    "else:\n",
    "    train_mean, train_std = ([0.4677, 0.4677, 0.4677]), ([0.2274, 0.2274, 0.2274])\n",
    "    val_mean, val_std = ([0.4718, 0.4718, 0.4718]), ([0.2260, 0.2260, 0.2260])\n",
    "    test_mean, test_std = ([0.4735, 0.4735, 0.4735]), ([0.2275, 0.2275, 0.2275])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"1.1+\"\"\"\n",
    "#The transform function for train data\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(img_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(train_mean, train_std)\n",
    "])\n",
    "\n",
    "#The transform function for validation data\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(val_mean, val_std)\n",
    "])\n",
    "\n",
    "#The transform function for test data\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(test_mean, test_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"1.2+\"\"\"\n",
    "trainset = torchvision.datasets.ImageFolder(root='../data/food/training', transform=transform_train)\n",
    "valset = torchvision.datasets.ImageFolder(root='../data/food/validation', transform=transform_val)\n",
    "testset = torchvision.datasets.ImageFolder(root='../data/food/evaluation', transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"1.3\"\"\"\n",
    "\n",
    "#Create DataLoader to draw samples from the dataset\n",
    "#In this case, we define a DataLoader to random sample our dataset. \n",
    "#For single sampling, we take one batch of data. Each batch consists 4 images\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "shuffle=True, num_workers=2)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=32,\n",
    "shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('Bread', 'DairyProduct', 'Dessert', 'Egg', 'Friedfood',\n",
    "           'Meat', 'Noodles-Pasta', 'Rice', 'Seafood', 'Soup', 'Vegetable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Building model..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define your own model\n",
    "class Net(nn.Module):\n",
    "\n",
    "    #define the layers\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * linear_size * linear_size, fc1_out)\n",
    "        self.fc2 = nn.Linear(fc1_out, fc2_out)\n",
    "        self.fc3 = nn.Linear(fc2_out, 11)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    #concatenate these layers\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        #x = self.dropout(x)\n",
    "        x = x.view(-1, 16 * linear_size * linear_size)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#declare a new model\n",
    "net = Net()\n",
    "#import torchvision.models as models\n",
    "#net = models.resnet18(pretrained=False)\n",
    "\n",
    "# change all model tensor into cuda type\n",
    "# something like weight & bias are the tensor \n",
    "#net = net.to(device)\n",
    "print(device)\n",
    "if device == 'cuda':\n",
    "    net = net.cuda(0)\n",
    "else:\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "\n",
    "# 3. Define a Loss function and optimize\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Defining loss function and optimize..\n"
     ]
    }
   ],
   "source": [
    "print('==> Defining loss function and optimize..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimization algorithm\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "\n",
    "# 4. Train the network\n",
    "\n",
    "Before training the model, we need to analysis the tensor variable.\n",
    "\n",
    "\n",
    "Each variable have many attibute, like: .grad_fn, .require_grad, .data, .grad...etc. The \".grad_fn\" attribute of \"torch.Tensor\" is an entry point into the function that has create this \"torch.Tensor\" variables. Because of \".grad_fn\" flag, we can easily create a computing graph in the form of DAG(directed acyclic graph).\n",
    "\n",
    "And then, the \".require_grad\" attribute allows us to determine whether the backward propagation function is going to calculate the gradient of this \"torch.Tensor\" variable. If one variable has a false value of require_grad, it represent that you don't want to calculate this variable's gradient, and also its gradient will not be updated.\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Training model..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "for phase in ['train', 'val']:\n",
    "    if phase == 'train':\n",
    "        print(phase)    \n",
    "    else:\n",
    "        print(phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Epoch train Loss: 0.0706 Acc: 0.1865 out of 9866\n",
      "0 Epoch val Loss: 0.0677 Acc: 0.2292 out of 3430\n",
      "1 Epoch train Loss: 0.0672 Acc: 0.2333 out of 9866\n",
      "1 Epoch val Loss: 0.0654 Acc: 0.2484 out of 3430\n",
      "2 Epoch train Loss: 0.0656 Acc: 0.2542 out of 9866\n",
      "2 Epoch val Loss: 0.0640 Acc: 0.2536 out of 3430\n",
      "3 Epoch train Loss: 0.0637 Acc: 0.2809 out of 9866\n",
      "3 Epoch val Loss: 0.0597 Acc: 0.3461 out of 3430\n",
      "4 Epoch train Loss: 0.0616 Acc: 0.3087 out of 9866\n",
      "4 Epoch val Loss: 0.0592 Acc: 0.3280 out of 3430\n",
      "5 Epoch train Loss: 0.0606 Acc: 0.3262 out of 9866\n",
      "5 Epoch val Loss: 0.0583 Acc: 0.3557 out of 3430\n",
      "6 Epoch train Loss: 0.0592 Acc: 0.3419 out of 9866\n",
      "6 Epoch val Loss: 0.0569 Acc: 0.3723 out of 3430\n",
      "7 Epoch train Loss: 0.0591 Acc: 0.3391 out of 9866\n",
      "7 Epoch val Loss: 0.0563 Acc: 0.3869 out of 3430\n",
      "8 Epoch train Loss: 0.0584 Acc: 0.3531 out of 9866\n",
      "8 Epoch val Loss: 0.0550 Acc: 0.3918 out of 3430\n",
      "9 Epoch train Loss: 0.0576 Acc: 0.3652 out of 9866\n",
      "9 Epoch val Loss: 0.0534 Acc: 0.4192 out of 3430\n",
      "10 Epoch train Loss: 0.0570 Acc: 0.3710 out of 9866\n",
      "10 Epoch val Loss: 0.0524 Acc: 0.4242 out of 3430\n",
      "11 Epoch train Loss: 0.0563 Acc: 0.3773 out of 9866\n",
      "11 Epoch val Loss: 0.0522 Acc: 0.4216 out of 3430\n",
      "12 Epoch train Loss: 0.0564 Acc: 0.3826 out of 9866\n",
      "12 Epoch val Loss: 0.0521 Acc: 0.4283 out of 3430\n",
      "13 Epoch train Loss: 0.0556 Acc: 0.3829 out of 9866\n",
      "13 Epoch val Loss: 0.0510 Acc: 0.4469 out of 3430\n",
      "14 Epoch train Loss: 0.0550 Acc: 0.3894 out of 9866\n",
      "14 Epoch val Loss: 0.0507 Acc: 0.4359 out of 3430\n",
      "15 Epoch train Loss: 0.0546 Acc: 0.3956 out of 9866\n",
      "15 Epoch val Loss: 0.0511 Acc: 0.4373 out of 3430\n",
      "16 Epoch train Loss: 0.0544 Acc: 0.3941 out of 9866\n",
      "16 Epoch val Loss: 0.0511 Acc: 0.4423 out of 3430\n",
      "17 Epoch train Loss: 0.0542 Acc: 0.3976 out of 9866\n",
      "17 Epoch val Loss: 0.0503 Acc: 0.4528 out of 3430\n",
      "18 Epoch train Loss: 0.0546 Acc: 0.3960 out of 9866\n",
      "18 Epoch val Loss: 0.0506 Acc: 0.4519 out of 3430\n",
      "19 Epoch train Loss: 0.0538 Acc: 0.4017 out of 9866\n",
      "19 Epoch val Loss: 0.0510 Acc: 0.4370 out of 3430\n",
      "20 Epoch train Loss: 0.0537 Acc: 0.4039 out of 9866\n",
      "20 Epoch val Loss: 0.0493 Acc: 0.4609 out of 3430\n",
      "21 Epoch train Loss: 0.0535 Acc: 0.4056 out of 9866\n",
      "21 Epoch val Loss: 0.0518 Acc: 0.4437 out of 3430\n",
      "22 Epoch train Loss: 0.0528 Acc: 0.4157 out of 9866\n",
      "22 Epoch val Loss: 0.0495 Acc: 0.4592 out of 3430\n",
      "23 Epoch train Loss: 0.0525 Acc: 0.4167 out of 9866\n",
      "23 Epoch val Loss: 0.0497 Acc: 0.4598 out of 3430\n",
      "24 Epoch train Loss: 0.0527 Acc: 0.4127 out of 9866\n",
      "24 Epoch val Loss: 0.0479 Acc: 0.4723 out of 3430\n",
      "25 Epoch train Loss: 0.0526 Acc: 0.4243 out of 9866\n",
      "25 Epoch val Loss: 0.0488 Acc: 0.4714 out of 3430\n",
      "26 Epoch train Loss: 0.0517 Acc: 0.4235 out of 9866\n",
      "26 Epoch val Loss: 0.0475 Acc: 0.4936 out of 3430\n",
      "27 Epoch train Loss: 0.0517 Acc: 0.4287 out of 9866\n",
      "27 Epoch val Loss: 0.0486 Acc: 0.4781 out of 3430\n",
      "28 Epoch train Loss: 0.0513 Acc: 0.4325 out of 9866\n",
      "28 Epoch val Loss: 0.0494 Acc: 0.4577 out of 3430\n",
      "29 Epoch train Loss: 0.0511 Acc: 0.4321 out of 9866\n",
      "29 Epoch val Loss: 0.0475 Acc: 0.4790 out of 3430\n",
      "30 Epoch train Loss: 0.0509 Acc: 0.4367 out of 9866\n",
      "30 Epoch val Loss: 0.0462 Acc: 0.4918 out of 3430\n",
      "31 Epoch train Loss: 0.0502 Acc: 0.4451 out of 9866\n",
      "31 Epoch val Loss: 0.0465 Acc: 0.4933 out of 3430\n",
      "32 Epoch train Loss: 0.0502 Acc: 0.4490 out of 9866\n",
      "32 Epoch val Loss: 0.0473 Acc: 0.4880 out of 3430\n",
      "33 Epoch train Loss: 0.0503 Acc: 0.4473 out of 9866\n",
      "33 Epoch val Loss: 0.0461 Acc: 0.5073 out of 3430\n",
      "34 Epoch train Loss: 0.0498 Acc: 0.4487 out of 9866\n",
      "34 Epoch val Loss: 0.0451 Acc: 0.5102 out of 3430\n",
      "35 Epoch train Loss: 0.0500 Acc: 0.4464 out of 9866\n",
      "35 Epoch val Loss: 0.0462 Acc: 0.5035 out of 3430\n",
      "36 Epoch train Loss: 0.0494 Acc: 0.4555 out of 9866\n",
      "36 Epoch val Loss: 0.0449 Acc: 0.5210 out of 3430\n",
      "37 Epoch train Loss: 0.0493 Acc: 0.4565 out of 9866\n",
      "37 Epoch val Loss: 0.0446 Acc: 0.5236 out of 3430\n",
      "38 Epoch train Loss: 0.0490 Acc: 0.4540 out of 9866\n",
      "38 Epoch val Loss: 0.0443 Acc: 0.5175 out of 3430\n",
      "39 Epoch train Loss: 0.0486 Acc: 0.4611 out of 9866\n",
      "39 Epoch val Loss: 0.0468 Acc: 0.5064 out of 3430\n",
      "40 Epoch train Loss: 0.0493 Acc: 0.4554 out of 9866\n",
      "40 Epoch val Loss: 0.0457 Acc: 0.5105 out of 3430\n",
      "41 Epoch train Loss: 0.0485 Acc: 0.4672 out of 9866\n",
      "41 Epoch val Loss: 0.0449 Acc: 0.5140 out of 3430\n",
      "42 Epoch train Loss: 0.0484 Acc: 0.4680 out of 9866\n",
      "42 Epoch val Loss: 0.0456 Acc: 0.5181 out of 3430\n",
      "43 Epoch train Loss: 0.0484 Acc: 0.4618 out of 9866\n",
      "43 Epoch val Loss: 0.0449 Acc: 0.5286 out of 3430\n",
      "44 Epoch train Loss: 0.0483 Acc: 0.4667 out of 9866\n",
      "44 Epoch val Loss: 0.0440 Acc: 0.5321 out of 3430\n",
      "45 Epoch train Loss: 0.0480 Acc: 0.4707 out of 9866\n",
      "45 Epoch val Loss: 0.0457 Acc: 0.5134 out of 3430\n",
      "46 Epoch train Loss: 0.0474 Acc: 0.4744 out of 9866\n",
      "46 Epoch val Loss: 0.0461 Acc: 0.5035 out of 3430\n",
      "47 Epoch train Loss: 0.0472 Acc: 0.4749 out of 9866\n",
      "47 Epoch val Loss: 0.0441 Acc: 0.5254 out of 3430\n",
      "48 Epoch train Loss: 0.0475 Acc: 0.4740 out of 9866\n",
      "48 Epoch val Loss: 0.0437 Acc: 0.5341 out of 3430\n",
      "49 Epoch train Loss: 0.0470 Acc: 0.4837 out of 9866\n",
      "49 Epoch val Loss: 0.0426 Acc: 0.5347 out of 3430\n",
      "50 Epoch train Loss: 0.0470 Acc: 0.4818 out of 9866\n",
      "50 Epoch val Loss: 0.0436 Acc: 0.5268 out of 3430\n",
      "51 Epoch train Loss: 0.0468 Acc: 0.4873 out of 9866\n",
      "51 Epoch val Loss: 0.0433 Acc: 0.5286 out of 3430\n",
      "52 Epoch train Loss: 0.0468 Acc: 0.4821 out of 9866\n",
      "52 Epoch val Loss: 0.0433 Acc: 0.5239 out of 3430\n",
      "53 Epoch train Loss: 0.0462 Acc: 0.4921 out of 9866\n",
      "53 Epoch val Loss: 0.0439 Acc: 0.5315 out of 3430\n",
      "54 Epoch train Loss: 0.0462 Acc: 0.4896 out of 9866\n",
      "54 Epoch val Loss: 0.0449 Acc: 0.5146 out of 3430\n",
      "55 Epoch train Loss: 0.0459 Acc: 0.4972 out of 9866\n",
      "55 Epoch val Loss: 0.0434 Acc: 0.5324 out of 3430\n",
      "56 Epoch train Loss: 0.0458 Acc: 0.4935 out of 9866\n",
      "56 Epoch val Loss: 0.0433 Acc: 0.5429 out of 3430\n",
      "57 Epoch train Loss: 0.0460 Acc: 0.4885 out of 9866\n",
      "57 Epoch val Loss: 0.0440 Acc: 0.5289 out of 3430\n",
      "58 Epoch train Loss: 0.0458 Acc: 0.4957 out of 9866\n",
      "58 Epoch val Loss: 0.0435 Acc: 0.5312 out of 3430\n",
      "59 Epoch train Loss: 0.0455 Acc: 0.4959 out of 9866\n",
      "59 Epoch val Loss: 0.0453 Acc: 0.5219 out of 3430\n",
      "60 Epoch train Loss: 0.0455 Acc: 0.4986 out of 9866\n",
      "60 Epoch val Loss: 0.0428 Acc: 0.5364 out of 3430\n",
      "61 Epoch train Loss: 0.0456 Acc: 0.4969 out of 9866\n",
      "61 Epoch val Loss: 0.0438 Acc: 0.5204 out of 3430\n",
      "62 Epoch train Loss: 0.0453 Acc: 0.5029 out of 9866\n",
      "62 Epoch val Loss: 0.0442 Acc: 0.5207 out of 3430\n",
      "63 Epoch train Loss: 0.0457 Acc: 0.4964 out of 9866\n",
      "63 Epoch val Loss: 0.0426 Acc: 0.5405 out of 3430\n",
      "64 Epoch train Loss: 0.0452 Acc: 0.5032 out of 9866\n",
      "64 Epoch val Loss: 0.0419 Acc: 0.5356 out of 3430\n",
      "65 Epoch train Loss: 0.0450 Acc: 0.5054 out of 9866\n",
      "65 Epoch val Loss: 0.0421 Acc: 0.5417 out of 3430\n",
      "66 Epoch train Loss: 0.0449 Acc: 0.5054 out of 9866\n",
      "66 Epoch val Loss: 0.0429 Acc: 0.5321 out of 3430\n",
      "67 Epoch train Loss: 0.0448 Acc: 0.5093 out of 9866\n",
      "67 Epoch val Loss: 0.0433 Acc: 0.5309 out of 3430\n",
      "68 Epoch train Loss: 0.0446 Acc: 0.5042 out of 9866\n",
      "68 Epoch val Loss: 0.0444 Acc: 0.5169 out of 3430\n",
      "69 Epoch train Loss: 0.0444 Acc: 0.5124 out of 9866\n",
      "69 Epoch val Loss: 0.0422 Acc: 0.5335 out of 3430\n",
      "70 Epoch train Loss: 0.0445 Acc: 0.5108 out of 9866\n",
      "70 Epoch val Loss: 0.0434 Acc: 0.5274 out of 3430\n",
      "71 Epoch train Loss: 0.0443 Acc: 0.5095 out of 9866\n",
      "71 Epoch val Loss: 0.0417 Acc: 0.5475 out of 3430\n",
      "72 Epoch train Loss: 0.0448 Acc: 0.5122 out of 9866\n",
      "72 Epoch val Loss: 0.0417 Acc: 0.5437 out of 3430\n",
      "73 Epoch train Loss: 0.0443 Acc: 0.5162 out of 9866\n",
      "73 Epoch val Loss: 0.0431 Acc: 0.5347 out of 3430\n",
      "74 Epoch train Loss: 0.0437 Acc: 0.5245 out of 9866\n",
      "74 Epoch val Loss: 0.0434 Acc: 0.5265 out of 3430\n",
      "75 Epoch train Loss: 0.0442 Acc: 0.5092 out of 9866\n",
      "75 Epoch val Loss: 0.0438 Acc: 0.5344 out of 3430\n",
      "76 Epoch train Loss: 0.0438 Acc: 0.5166 out of 9866\n",
      "76 Epoch val Loss: 0.0420 Acc: 0.5504 out of 3430\n",
      "77 Epoch train Loss: 0.0438 Acc: 0.5222 out of 9866\n",
      "77 Epoch val Loss: 0.0427 Acc: 0.5417 out of 3430\n",
      "78 Epoch train Loss: 0.0437 Acc: 0.5243 out of 9866\n",
      "78 Epoch val Loss: 0.0420 Acc: 0.5449 out of 3430\n",
      "79 Epoch train Loss: 0.0432 Acc: 0.5256 out of 9866\n",
      "79 Epoch val Loss: 0.0425 Acc: 0.5507 out of 3430\n",
      "80 Epoch train Loss: 0.0431 Acc: 0.5259 out of 9866\n",
      "80 Epoch val Loss: 0.0411 Acc: 0.5601 out of 3430\n",
      "81 Epoch train Loss: 0.0432 Acc: 0.5264 out of 9866\n",
      "81 Epoch val Loss: 0.0413 Acc: 0.5469 out of 3430\n",
      "82 Epoch train Loss: 0.0431 Acc: 0.5264 out of 9866\n",
      "82 Epoch val Loss: 0.0417 Acc: 0.5542 out of 3430\n",
      "83 Epoch train Loss: 0.0432 Acc: 0.5260 out of 9866\n",
      "83 Epoch val Loss: 0.0410 Acc: 0.5612 out of 3430\n",
      "84 Epoch train Loss: 0.0431 Acc: 0.5193 out of 9866\n",
      "84 Epoch val Loss: 0.0424 Acc: 0.5376 out of 3430\n",
      "85 Epoch train Loss: 0.0428 Acc: 0.5222 out of 9866\n",
      "85 Epoch val Loss: 0.0431 Acc: 0.5426 out of 3430\n",
      "86 Epoch train Loss: 0.0422 Acc: 0.5403 out of 9866\n",
      "86 Epoch val Loss: 0.0408 Acc: 0.5580 out of 3430\n",
      "87 Epoch train Loss: 0.0426 Acc: 0.5352 out of 9866\n",
      "87 Epoch val Loss: 0.0405 Acc: 0.5671 out of 3430\n",
      "88 Epoch train Loss: 0.0430 Acc: 0.5342 out of 9866\n",
      "88 Epoch val Loss: 0.0432 Acc: 0.5356 out of 3430\n",
      "89 Epoch train Loss: 0.0431 Acc: 0.5329 out of 9866\n",
      "89 Epoch val Loss: 0.0416 Acc: 0.5478 out of 3430\n",
      "90 Epoch train Loss: 0.0427 Acc: 0.5262 out of 9866\n",
      "90 Epoch val Loss: 0.0429 Acc: 0.5496 out of 3430\n",
      "91 Epoch train Loss: 0.0427 Acc: 0.5371 out of 9866\n",
      "91 Epoch val Loss: 0.0415 Acc: 0.5496 out of 3430\n",
      "92 Epoch train Loss: 0.0425 Acc: 0.5362 out of 9866\n",
      "92 Epoch val Loss: 0.0401 Acc: 0.5708 out of 3430\n",
      "93 Epoch train Loss: 0.0423 Acc: 0.5338 out of 9866\n",
      "93 Epoch val Loss: 0.0420 Acc: 0.5554 out of 3430\n",
      "94 Epoch train Loss: 0.0422 Acc: 0.5373 out of 9866\n",
      "94 Epoch val Loss: 0.0416 Acc: 0.5458 out of 3430\n",
      "95 Epoch train Loss: 0.0421 Acc: 0.5374 out of 9866\n",
      "95 Epoch val Loss: 0.0421 Acc: 0.5443 out of 3430\n",
      "96 Epoch train Loss: 0.0424 Acc: 0.5344 out of 9866\n",
      "96 Epoch val Loss: 0.0412 Acc: 0.5598 out of 3430\n",
      "97 Epoch train Loss: 0.0422 Acc: 0.5384 out of 9866\n",
      "97 Epoch val Loss: 0.0425 Acc: 0.5364 out of 3430\n",
      "98 Epoch train Loss: 0.0414 Acc: 0.5469 out of 9866\n",
      "98 Epoch val Loss: 0.0415 Acc: 0.5458 out of 3430\n",
      "99 Epoch train Loss: 0.0419 Acc: 0.5343 out of 9866\n",
      "99 Epoch val Loss: 0.0427 Acc: 0.5364 out of 3430\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Set the model in training mode\n",
    "#because some function like: dropout, batchnorm...etc, will have \n",
    "#different behaviors in training/evaluation mode\n",
    "#[document]: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train\n",
    "import copy\n",
    "\n",
    "best_model = net\n",
    "best_acc = 0.0\n",
    "\n",
    "net.train()\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma=0.5)\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    #scheduler.step()\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            net.train(True)  # Set model to training mode\n",
    "            dset_loaders = trainloader\n",
    "        else:\n",
    "            net.train(False)  # Set model to evaluate mode\n",
    "            dset_loaders = valloader\n",
    "            \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        \n",
    "        for data in dset_loaders:\n",
    "        \n",
    "            (inputs, labels) = data\n",
    "            \n",
    "            #change the type into cuda tensor \n",
    "            if device == 'cuda':\n",
    "                inputs = inputs.cuda(0)\n",
    "                labels = labels.cuda(0)\n",
    "            else:\n",
    "                inputs = inputs.cpu()\n",
    "                labels = labels.cpu()\n",
    "\n",
    "            #print(labels)\n",
    "            #print(inputs)\n",
    "        \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            # select the class with highest probability\n",
    "            _, pred = outputs.max(1)\n",
    "            # if the model predicts the same results as the true\n",
    "            # label, then the correct counter will plus 1\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if phase == 'train':\n",
    "            dataset_size = len(trainset)\n",
    "        else:\n",
    "            dataset_size = len(valset)\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_acc = correct / dataset_size\n",
    "\n",
    "        print('{:d} Epoch {} Loss: {:.4f} Acc: {:.4f} out of {:d}'\n",
    "              .format(epoch+1, phase, epoch_loss, epoch_acc, dataset_size))\n",
    "\n",
    "        # deep copy the model\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(net)\n",
    "print('Finished Training')\n",
    "\n",
    "net = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "def show_weight_distribution(epoch):\n",
    "    n_bins = 50\n",
    "    font_size = 8\n",
    "    plt.rcParams.update({'axes.titlesize': font_size})\n",
    "    plt.rcParams.update({'axes.labelsize': font_size})\n",
    "    plt.rcParams.update({'xtick.labelsize': font_size})\n",
    "    plt.rcParams.update({'ytick.labelsize': font_size})\n",
    "    fig, axs = plt.subplots(1, 5, sharey=True)\n",
    "    \n",
    "    x = net.conv1.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('conv1')\n",
    "    plt.title('Weight Distribution, epoch: %d' %(epoch))\n",
    "\n",
    "    x = net.conv2.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('conv2')\n",
    "\n",
    "    x = net.fc1.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc1')\n",
    "\n",
    "    x = net.fc2.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc2')\n",
    "\n",
    "    x = net.fc3.weight.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc3')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "def show_bias_distribution(epoch):\n",
    "    n_bins = 50\n",
    "    font_size = 8\n",
    "    plt.rcParams.update({'axes.titlesize': font_size})\n",
    "    plt.rcParams.update({'axes.labelsize': font_size})\n",
    "    plt.rcParams.update({'xtick.labelsize': font_size})\n",
    "    plt.rcParams.update({'ytick.labelsize': font_size})\n",
    "    fig, axs = plt.subplots(1, 5, sharey=True)\n",
    "    \n",
    "    x = net.conv1.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('conv1')\n",
    "    plt.title('Bias Distribution, epoch: %d' %(epoch))\n",
    "\n",
    "    x = net.conv2.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 2)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('conv2')\n",
    "\n",
    "    x = net.fc1.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 3)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc1')\n",
    "\n",
    "    x = net.fc2.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 4)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc2')\n",
    "\n",
    "    x = net.fc3.bias.view(-1,1).cpu().detach().numpy()\n",
    "    plt.subplot(1, 5, 5)\n",
    "    plt.hist(x, bins=n_bins)\n",
    "    plt.xlabel('fc3')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'conv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f17b1820c3fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_weight_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_bias_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-83fc42c55b8e>\u001b[0m in \u001b[0;36mshow_weight_distribution\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Weight Distribution, epoch: %d'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ccma/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'conv2'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAERCAYAAACZystaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGQaSJMNAkoRhIEnCMJAkYRhIkoD/B5uc2CVc7IjOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_weight_distribution(epoch+1)\n",
    "show_bias_distribution(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#After training , save the model first\n",
    "#You can saves only the model parameters or entire model\n",
    "#Some difference between the two is that entire model \n",
    "#not only include parameters but also record hwo each \n",
    "#layer is connected(forward method).\n",
    "#[document]: https://pytorch.org/docs/master/notes/serialization.html\n",
    "\n",
    "print('==> Saving model..')\n",
    "\n",
    "#only save model parameters\n",
    "torch.save(net.state_dict(), './checkpoint.t7')\n",
    "#you also can store some log information\n",
    "state = {\n",
    "    'net': net.state_dict(),\n",
    "    'acc': 100.*correct/len(trainset),\n",
    "    'epoch': 75\n",
    "}\n",
    "torch.save(state, './checkpoint.t7')\n",
    "\n",
    "#save entire model\n",
    "torch.save(net, './model.pt')\n",
    "\n",
    "print('Finished Saving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################################################\n",
    "\n",
    "# 5. Test the network on the test data\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Before testing, we can load the saved model\n",
    "#Depend on how you save your model, need \n",
    "#different way to use it\n",
    "\n",
    "print('==> Loading model..')\n",
    "\n",
    "#If you just save the model parameters, you\n",
    "#need to redefine the model architecture, and\n",
    "#load the parameters into your model\n",
    "net = Net()\n",
    "checkpoint = torch.load('./checkpoint.t7')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "\n",
    "#If you save the entire model\n",
    "net = torch.load('./model.pt')\n",
    "\n",
    "print('Finished Loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('==> Testing model..')\n",
    "\n",
    "#Set the model in evaluation mode\n",
    "#[document]: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval \n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#######LAB 1-1########\n",
    "\"\"\"\n",
    "To Do Here\n",
    "\n",
    "        You should complete the testing step in LAB 1-1\n",
    "You should show the total accuracy & loss [10000 cifar10 test cases]\n",
    "  You also need to tell us, how about the accuracy for each class\n",
    " \n",
    "    For example: Total accuracy is: 60.0% and loss is: 0.02  \n",
    "                 For each class in cifar 10:\n",
    "                 Accuracy of plane : 58.0%\n",
    "                 Accuracy of   car : 22.4%\n",
    "                            .\n",
    "                            .\n",
    "                            .\n",
    "\"\"\"\n",
    "######################\n",
    "correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "class_correct = list(0. for i in range(11))\n",
    "class_total = list(0. for i in range(11))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "\n",
    "        if device == 'cuda':\n",
    "            images = images.cuda(0)\n",
    "            labels = labels.cuda(0)\n",
    "        else:\n",
    "            images = images.cpu()\n",
    "            labels = labels.cpu()\n",
    "        \n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        c = (predicted == labels).squeeze()\n",
    "        #print(predicted)\n",
    "        #print(labels.size(0))\n",
    "        #print(c)\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print('Accuracy of the network on the %d test images: %.2f%%, and loss is: %.3f'\n",
    "      % (total, 100 * correct / total, running_loss / total))\n",
    "\n",
    "for i in range(11):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
